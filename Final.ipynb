{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_output(better = [False, \"noun\", 5]):\n",
    "    if better[0]:\n",
    "        y = pd.read_csv(\"better/better_train_{}_tags_{}.csv\".format(better[2], better[1]))\n",
    "    else:\n",
    "        y = pd.read_csv(\"processed_tags.csv\")\n",
    "        y.drop([\"Name\"], axis=1, inplace = True)\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data(bad_tags = [False, \"noun\"], fc = False, pool = False):\n",
    "    if bad_tags[0]:\n",
    "        train_lines = np.array([' '.join(f.split()) for i in range(10000) \n",
    "                      for f in open(\"features_train_prepossesing_{}/\".format(bad_tags[1])\n",
    "                                    + str(i) \n",
    "                                    + \".txt\")])\n",
    "        unique_words = list(set([item for sublist in train_lines.ravel() for item in sublist.split()]))\n",
    "\n",
    "        count_vect = TfidfVectorizer(vocabulary = unique_words)\n",
    "        X_train = count_vect.fit_transform(train_lines)\n",
    "        X_train = X_train.toarray()\n",
    "\n",
    "        test_lines = np.array([' '.join(f.split()) for i in range(2000) \n",
    "                      for f in open(\"features_test_prepossesing_{}/\".format(bad_tags[1])\n",
    "                                    + str(i) \n",
    "                                    + \".txt\")])\n",
    "\n",
    "        X_test = count_vect.transform(test_lines)\n",
    "        X_test = X_test.toarray()\n",
    "\n",
    "    elif fc:\n",
    "        X_train = pd.read_csv(\"data/features_train/features_resnet1000_train.csv\", \n",
    "                              header = None, names = [\"Neuron_\" + str(i) for i in range(1000)])\n",
    "        indices = X_train.index\n",
    "        new_indices = [int(re.findall(r'\\d+', ind)[0]) for ind in indices]\n",
    "        X_train = np.array(pd.DataFrame(np.array(X_train), columns=X_train.columns, index=new_indices).sort_index())\n",
    "        \n",
    "        X_test = pd.read_csv(\"data/features_test/features_resnet1000_test.csv\", \n",
    "                             header = None, names = [\"Neuron_\" + str(i) for i in range(1000)])\n",
    "        indices = X_test.index\n",
    "        new_indices = [int(re.findall(r'\\d+', ind)[0]) for ind in indices]\n",
    "        X_test = np.array(pd.DataFrame(np.array(X_test), columns=X_test.columns, index=new_indices).sort_index())\n",
    "    \n",
    "    elif pool:\n",
    "        X_train = pd.read_csv(\"data/features_train/features_resnet1000intermediate_train.csv\", \n",
    "                              header = None, names = [\"Neuron_\" + str(i) for i in range(2048)])\n",
    "        indices = X_train.index\n",
    "        new_indices = [int(re.findall(r'\\d+', ind)[0]) for ind in indices]\n",
    "        X_train = np.array(pd.DataFrame(np.array(X_train), columns=X_train.columns, index=new_indices).sort_index())\n",
    "        \n",
    "        X_test = pd.read_csv(\"data/features_test/features_resnet1000intermediate_test.csv\", \n",
    "                             header = None, names = [\"Neuron_\" + str(i) for i in range(2048)])\n",
    "        indices = X_test.index\n",
    "        new_indices = [int(re.findall(r'\\d+', ind)[0]) for ind in indices]\n",
    "        X_test = np.array(pd.DataFrame(np.array(X_test), columns=X_test.columns, index=new_indices).sort_index())\n",
    "    \n",
    "    else:\n",
    "        print(\"Set Parameters\")\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_better_tags(k, words_type = \"noun\", product=True):\n",
    "    X_train, X_test = get_data(bad_tags = [True, words_type])\n",
    "    \n",
    "    train_lines = np.array([' '.join(f.split()) for i in range(10000) \n",
    "                      for f in open(\"features_train_prepossesing_{}/\".format(words_type)\n",
    "                                    + str(i) \n",
    "                                    + \".txt\")])\n",
    "    \n",
    "    unique_words = list(set([item for sublist in train_lines.ravel() for item in sublist.split()]))\n",
    "    \n",
    "    unique_tags = []\n",
    "    for arr in X_train:\n",
    "        tags = [i for i in arr.argsort()[-k:][::-1]]\n",
    "\n",
    "        for i in tags:\n",
    "            if unique_words[i] not in unique_tags:\n",
    "                unique_tags.append(unique_words[i])\n",
    "    \n",
    "    print(len(unique_tags))\n",
    "    \n",
    "    d = dict([(tag, 0) for tag in unique_tags])\n",
    "    \n",
    "    better_tags = []\n",
    "    for arr in X_train:\n",
    "        d_sample = d.copy()\n",
    "\n",
    "        for i in arr.argsort():\n",
    "            if (arr[i]>0) and (unique_words[i] in unique_tags):\n",
    "                d_sample[unique_words[i]] = arr[i]\n",
    "        better_tags.append(list(d_sample.values()))\n",
    "\n",
    "    print(len(better_tags[0]))\n",
    "\n",
    "    pd.DataFrame(better_tags, \n",
    "                 columns=unique_tags).to_csv(\"better/better_train_{}_tags_{}.csv\".format(k, words_type), \n",
    "                                                          index=False)\n",
    "    \n",
    "    better_tags = []\n",
    "    for arr in X_test:\n",
    "        d_sample = d.copy()\n",
    "\n",
    "        for i in arr.argsort():\n",
    "            if (arr[i]>0) and (unique_words[i] in unique_tags):\n",
    "                d_sample[unique_words[i]] = arr[i]\n",
    "        better_tags.append(list(d_sample.values()))\n",
    "\n",
    "    print(len(better_tags[0]))\n",
    "\n",
    "    pd.DataFrame(better_tags, columns=unique_tags).to_csv(\"better/better_test_{}_tags_{}.csv\".format(k, words_type), \n",
    "                                                          index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_nn(X_train, y, add_tags=True):\n",
    "    if add_tags:\n",
    "        tags_train = pd.read_csv(\"processed_tags.csv\")\n",
    "        tags_train.drop([\"Name\"], axis=1, inplace = True)\n",
    "        tags_train = np.array(tags_train)\n",
    "\n",
    "        X_train = np.hstack((X_train, tags_train))\n",
    "\n",
    "    print(\"# of input features {}\".format(X_train.shape[1]))\n",
    "        \n",
    "    folds = 3\n",
    "    skf = KFold(n_splits=folds)\n",
    "    accuracy = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_train, y):\n",
    "        print(\"SPLIT\")\n",
    "        x_train, x_test = X_train[train_index], X_train[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(500, input_dim=x_train.shape[1], init='normal', activation='relu'))\n",
    "#         model.add(Dropout(0.2))\n",
    "        model.add(Dense(y_train.shape[1], init='normal', activation='softmax'))\n",
    "\n",
    "        # Compile model     #logarithmic  loss     #method\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(x_train, y_train, \n",
    "                  validation_data=(x_test, y_test),\n",
    "                  nb_epoch=15, batch_size=100,\n",
    "                  verbose=1)\n",
    "        \n",
    "        accuracy.append(model.evaluate(x_test, y_test)[1])\n",
    "        print()\n",
    "\n",
    "    print(\"Results\")\n",
    "    print(\"The average accuracy Neural Network is {}\".format(np.mean(accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_better_tags() got an unexpected keyword argument 'product'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a0e1405deaa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_better_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adj&noun&verb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: make_better_tags() got an unexpected keyword argument 'product'"
     ]
    }
   ],
   "source": [
    "make_better_tags(10, words_type=\"adj&noun&verb\", product=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = get_data(bad_tags = [True, \"adj&noun&verb\"])\n",
    "\n",
    "train_lines = np.array([' '.join(f.split()) for i in range(10000) \n",
    "                  for f in open(\"features_train_prepossesing_{}/\".format(\"adj&noun&verb\")\n",
    "                                + str(i) \n",
    "                                + \".txt\")])\n",
    "\n",
    "unique_words = list(set([item for sublist in train_lines.ravel() for item in sublist.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7771\n"
     ]
    }
   ],
   "source": [
    "unique_tags = []\n",
    "for arr in X_train:\n",
    "    tags = [i for i in arr.argsort()[-10:][::-1]]\n",
    "\n",
    "    for i in tags:\n",
    "        if unique_words[i] not in unique_tags:\n",
    "            unique_tags.append(unique_words[i])\n",
    "\n",
    "print(len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7771\n"
     ]
    }
   ],
   "source": [
    "unique_tags = []\n",
    "for arr in X_train:\n",
    "    tags = [i for i in arr.argsort()[-10:][::-1]]\n",
    "\n",
    "    for i in tags:\n",
    "        if unique_words[i] not in unique_tags:\n",
    "            unique_tags.append(unique_words[i])\n",
    "\n",
    "print(len(unique_tags))\n",
    "\n",
    "d = dict([(tag, 0) for tag in unique_tags])\n",
    "\n",
    "better_tags = []\n",
    "for arr in X_train:\n",
    "    d_sample = d.copy()\n",
    "\n",
    "    for i in arr.argsort():\n",
    "        if (arr[i]>0) and (unique_words[i] in unique_tags):\n",
    "            d_sample[unique_words[i]] = arr[i]\n",
    "    better_tags.append(list(d_sample.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60388441"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(better_tags[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.65366438058304799,\n",
       " 0.38684613176860422,\n",
       " 0.26856400428191401,\n",
       " 0.24580630343919188,\n",
       " 0.23046369082399695,\n",
       " 0.1857727303226801,\n",
       " 0.17744107005270038,\n",
       " 0.17051461233478074,\n",
       " 0.15802486846267014,\n",
       " 0.14282087538639421,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.062562594500189306,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.119160686290293,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.13751586998008378,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.11392829149281537,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.07503744644926455,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.10222221910794117,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.12536768425745751,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outbetter_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test_rf(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train, X_test = get_data(pool=True)\n",
    "# y = get_output(better=[True, \"adj&noun\", 5])\n",
    "\n",
    "# test_nn(X_train, y, add_tags=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nn_predict_description(add_tags=False, better=[True, \"adj&noun\", 5]):\n",
    "    X_train, X_test = get_data(pool=True)\n",
    "    y = get_output(better=better)\n",
    "    \n",
    "    if add_tags:\n",
    "        tags_train = pd.read_csv(\"processed_tags.csv\")\n",
    "        tags_train.drop([\"Name\"], axis=1, inplace = True)\n",
    "        tags_train = np.array(tags_train)\n",
    "               \n",
    "        X_train = np.hstack((X_train, tags_train))\n",
    "        \n",
    "        \n",
    "        tags_test = pd.read_csv(\"processed_tags_test.csv\")\n",
    "        tags_test.drop([\"Name\"], axis=1, inplace = True)\n",
    "        tags_test = np.array(tags_test)\n",
    "                \n",
    "        X_test = np.hstack((X_test, tags_test))\n",
    "    \n",
    "    print(\"# of input features {}\".format(X_train.shape[1]))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=X_train.shape[1], init='normal', activation='relu'))\n",
    "    model.add(Dense(y.shape[1], init='normal', activation='softmax'))\n",
    "    \n",
    "    # Compile model     #logarithmic  loss     #method\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y, \n",
    "              nb_epoch=13, batch_size=100,\n",
    "              verbose=1)\n",
    "\n",
    "    predicted_tags = model.predict(X_test)\n",
    "    \n",
    "    columns = pd.read_csv(\"better/better_train_{}_tags_{}.csv\".format(better[2], better[1])).columns\n",
    "    \n",
    "    pd.DataFrame(predicted_tags, \n",
    "                 columns=columns).to_csv(\"Predicted/CNN_{}_{}_predicted_tags.csv\".format(better[2], better[1]), \n",
    "                                         index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def MLP_predict_description(add_tags=False, better=[True, \"adj&noun\", 5]):\n",
    "    X_train, X_test = get_data(pool=True)\n",
    "    y = get_output(better=better)\n",
    "    \n",
    "    if add_tags:\n",
    "        tags_train = pd.read_csv(\"processed_tags.csv\")\n",
    "        tags_train.drop([\"Name\"], axis=1, inplace = True)\n",
    "        tags_train = np.array(tags_train)\n",
    "               \n",
    "        X_train = np.hstack((X_train, tags_train))\n",
    "        \n",
    "        tags_test = pd.read_csv(\"processed_tags_test.csv\")\n",
    "        tags_test.drop([\"Name\"], axis=1, inplace = True)\n",
    "        tags_test = np.array(tags_test)\n",
    "                \n",
    "        X_test = np.hstack((X_test, tags_test))\n",
    "    \n",
    "    y[y != 0] = 1\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=(500), verbose=True, max_iter=30, warm_start=True)\n",
    "    model.fit(X_train, y)\n",
    "    \n",
    "    predicted_tags = model.predict(X_test)\n",
    "    \n",
    "    columns = pd.read_csv(\"better/better_train_{}_tags_{}.csv\".format(better[2], better[1])).columns\n",
    "    \n",
    "    pd.DataFrame(predicted_tags, \n",
    "                 columns=columns).to_csv(\"Predicted/MLP_{}_{}_predicted_tags.csv\".format(better[2], better[1]), \n",
    "                                         index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_output(mlp=False, nn=False, pls=False, better=[True, \"adj&noun\", 5]):\n",
    "    \n",
    "    if nn == True:\n",
    "        groud_truth = pd.read_csv(\"better/better_test_{}_tags_{}.csv\".format(better[2], better[1]))\n",
    "        predicted = pd.read_csv(\"Predicted/CNN_{}_{}_predicted_tags.csv\".format(better[2], better[1]))\n",
    "\n",
    "        simi = cdist(groud_truth, predicted, metric='cosine')\n",
    "        pd.DataFrame(simi, \n",
    "                     columns=list(range(2000))).to_csv(\"similarity/NN_{}_pool+tags_similarity.csv\".format(better[1]), \n",
    "                                                             index=False)\n",
    "        \n",
    "        rows = []\n",
    "        k=0\n",
    "        for row in simi:\n",
    "            top20 = row.argsort()[:20]\n",
    "            rows.append([str(k) + \".txt\", \" \".join([str(value) + \".jpg\" for value in top20])])\n",
    "            k+=1\n",
    "        pd.DataFrame(rows, \n",
    "                     columns=['Descritpion_ID', 'Top_20_Image_IDs']).to_csv(\"Submissions/CNN_{}_submission.csv\".format(better[1]), \n",
    "                                                                            index=False)\n",
    "    elif mlp == True:\n",
    "        groud_truth = pd.read_csv(\"better/better_test_{}_tags_{}.csv\".format(better[2], better[1]))\n",
    "        predicted = pd.read_csv(\"Predicted/MLP_{}_{}_predicted_tags.csv\".format(better[2], better[1]))\n",
    "\n",
    "        simi = cdist(groud_truth, predicted, metric='cosine')\n",
    "        pd.DataFrame(simi, \n",
    "                     columns=list(range(2000))).to_csv(\"similarity/MLP_{}_pool+tags_similarity.csv\".format(better[1]), \n",
    "                                                             index=False)\n",
    "        \n",
    "        rows = []\n",
    "        k=0\n",
    "        for row in simi:\n",
    "            top20 = row.argsort()[:20]\n",
    "            rows.append([str(k) + \".txt\", \" \".join([str(value) + \".jpg\" for value in top20])])\n",
    "            k+=1\n",
    "        pd.DataFrame(rows, \n",
    "                     columns=['Descritpion_ID', 'Top_20_Image_IDs']).to_csv(\"Submissions/MLP_{}_submission.csv\".format(better[1]), \n",
    "                                                                            index=False)\n",
    "    elif pls == True:\n",
    "        groud_truth = pd.read_csv(\"better/better_test_{}_tags_{}.csv\".format(better[2], better[1]))\n",
    "        predicted = pd.read_csv(\"Predicted/PLS_{}_{}_predicted_tags.csv\".format(better[2], better[1]))\n",
    "\n",
    "        simi = cdist(groud_truth, predicted, metric='cosine')\n",
    "        pd.DataFrame(simi, \n",
    "                     columns=list(range(2000))).to_csv(\"similarity/PLS_{}_fc_similarity.csv\".format(better[1]), \n",
    "                                                             index=False)\n",
    "        \n",
    "        rows = []\n",
    "        k=0\n",
    "        for row in simi:\n",
    "            top20 = row.argsort()[:20]\n",
    "            rows.append([str(k) + \".txt\", \" \".join([str(value) + \".jpg\" for value in top20])])\n",
    "            k+=1\n",
    "        pd.DataFrame(rows, \n",
    "                     columns=['Descritpion_ID', 'Top_20_Image_IDs']).to_csv(\"Submissions/PLS_{}_submission.csv\".format(better[1]), \n",
    "                                                                            index=False)\n",
    "    else:\n",
    "        1+1\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_enseble(models):\n",
    "    matrices = []\n",
    "    for key, weight in models.items():\n",
    "        if key == \"best\":\n",
    "            d = pd.read_csv(\"similarity/NN_best_pool+tags_similarity.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "        elif key == \"PLS\":\n",
    "            d = pd.read_csv(\"similarity/{}_{}_fc_similarity.csv\".format(key, weight[0]))\n",
    "        elif key == \"TOP\":\n",
    "            d = pd.read_csv(\"similarity/NN_merge_simi.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "        else:\n",
    "            d = pd.read_csv(\"similarity/{}_{}_pool+tags_similarity.csv\".format(key, weight[0]))\n",
    "        matrices.append(weight[1]*np.array(d))\n",
    "\n",
    "    matrices = np.array(matrices)\n",
    "    \n",
    "    simi = sum(matrices)\n",
    "    \n",
    "    images = []\n",
    "    k=0\n",
    "    for row in simi:\n",
    "        top20 = row.argsort()[:20]\n",
    "        images.append([str(k) + \".txt\", \" \".join([str(value) + \".jpg\" for value in top20])])\n",
    "        k+=1\n",
    "        \n",
    "    pd.DataFrame(images, \n",
    "                 columns=['Descritpion_ID', 'Top_20_Image_IDs']).to_csv(\"Submissions/EN_{}_submission.csv\".format(list(models.keys())), \n",
    "                                                                        index=False)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# MLP_predict_description(add_tags=True, better=[True, \"adj&noun\", 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# nn_predict_description(add_tags=True, better=[True, \"adj&noun\", 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# images = make_output(mlp=True, better=[True, \"adj&noun\", 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# images = make_enseble({ \"PLS\": [\"adj&noun&verb\", 0.15],\n",
    "#                       \"TOP\": [\"\", 0.85]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# [ 0.86773841  0.06454957]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# image_to_show = 8\n",
    "\n",
    "# output_statement = images[image_to_show][1]\n",
    "\n",
    "# row, col = 5,5\n",
    "\n",
    "# f, ax = plt.subplots(nrows=row, ncols=col, figsize=(30,30))\n",
    "# for index, image_1 in enumerate(output_statement.split()):\n",
    "#     if index == row*col:\n",
    "#         break\n",
    "#     img=mpimg.imread('data/images_test/' + image_1)\n",
    "#     ax[int(index/row), index%col].set_title(image_1, fontsize=25)\n",
    "#     ax[int(index/row), index%col].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "def PLS_predict_description(better):\n",
    "    X_train, X_test = get_data(pool=True)\n",
    "    y = get_output(better=better)\n",
    "    \n",
    "    y[y != 0] = 1\n",
    "\n",
    "    model = PLSRegression(n_components=200)\n",
    "    model.fit(X_train, y)\n",
    "    \n",
    "    predicted_tags = model.predict(X_test)\n",
    "    \n",
    "    columns = pd.read_csv(\"better/better_train_{}_tags_{}.csv\".format(better[2], better[1])).columns\n",
    "    \n",
    "    pd.DataFrame(predicted_tags, \n",
    "                 columns=columns).to_csv(\"Predicted/PLS_{}_{}_predicted_tags.csv\".format(better[2], better[1]), \n",
    "                                         index=False)\n",
    "    \n",
    "# PLS_predict_description(better=[True, \"adj&noun&verb\", 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# images = make_output(pls=True, better=[True, \"adj&noun&verb\", 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# image_to_show = 2\n",
    "\n",
    "# output_statement = images[image_to_show][1]\n",
    "\n",
    "# row, col = 5,5\n",
    "\n",
    "# f, ax = plt.subplots(nrows=row, ncols=col, figsize=(30,30))\n",
    "# for index, image_1 in enumerate(output_statement.split()):\n",
    "#     if index == row*col:\n",
    "#         break\n",
    "#     img=mpimg.imread('data/images_test/' + image_1)\n",
    "#     ax[int(index/row), index%col].set_title(image_1, fontsize=25)\n",
    "#     ax[int(index/row), index%col].imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def get_simi_for_deb():\n",
    "    X_train, X_test = get_data(pool=True)\n",
    "    y = get_output(better=[True, \"adj&noun\", 5])\n",
    "\n",
    "    tags_train = pd.read_csv(\"processed_tags.csv\")\n",
    "    tags_train.drop([\"Name\"], axis=1, inplace = True)\n",
    "    tags_train = np.array(tags_train)\n",
    "\n",
    "    X_train = np.hstack((X_train, tags_train))\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train, columns=list(range(X_train.shape[1])))\n",
    "    \n",
    "    X_Train, X_Val, Y_train, Y_Val = train_test_split(X_train, y, train_size=0.8)\n",
    "    \n",
    "    gt_files = list(X_Val.index)\n",
    "    \n",
    "    file = open(\"Tunning/gt.txt\",\"w\") \n",
    " \n",
    "    for g in gt_files:\n",
    "        file.write(str(g)+\",\")\n",
    "\n",
    "    file.close() \n",
    "    \n",
    "    better_tag_val_data = Y_Val.copy()\n",
    "\n",
    "    X_Train = np.array(X_Train)\n",
    "    X_Val = np.array(X_Val)\n",
    "    Y_train[Y_train != 0] = 1\n",
    "    Y_Val[Y_Val != 0] = 1\n",
    "     \n",
    "    classifier = MLPClassifier(hidden_layer_sizes=(500), verbose=True, max_iter=30, warm_start=True)\n",
    "    classifier.fit(X_Train, Y_train)\n",
    "    \n",
    "    simi = cdist(better_tag_val_data, classifier.predict_proba(X_Val), metric='cosine')\n",
    "    pd.DataFrame(simi, columns=list(range(simi.shape[1]))).to_csv(\"Tunning/best.csv\", index=False)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=X_Train.shape[1], init='normal', activation='relu'))\n",
    "    model.add(Dense(Y_train.shape[1], init='normal', activation='softmax'))\n",
    "\n",
    "    # Compile model     #logarithmic  loss     #method\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_Train, Y_train, \n",
    "              nb_epoch=13, batch_size=100,\n",
    "              verbose=1)\n",
    "\n",
    "    predicted_tags = model.predict(X_Val)\n",
    "\n",
    "    simi = cdist(better_tag_val_data, predicted_tags, metric='cosine')\n",
    "    pd.DataFrame(simi, columns=list(range(simi.shape[1]))).to_csv(\"Tunning/NN.csv\", index=False)\n",
    "    \n",
    "    return gt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimize():\n",
    "    def get_matrices(models):\n",
    "        matrices = []\n",
    "        for key in models:\n",
    "            matrices.append(np.array(pd.read_csv(\"Tunning/{}.csv\".format(key))))\n",
    "\n",
    "        matrices = np.array(matrices)\n",
    "\n",
    "        return matrices\n",
    "    \n",
    "    def get_accuracy_on_set(simi):\n",
    "        \n",
    "        gt_files = open(\"Tunning/gt.txt\", \"r\").readlines()[0]\n",
    "        \n",
    "        gt_files = gt_files.split(\",\")[:-1]\n",
    "        \n",
    "        score = 0.0\n",
    "        for i_check in range(len(simi)):\n",
    "            pred_list = list(simi[i_check].argsort()[:20])\n",
    "\n",
    "            k = [gt_files[i] for i in pred_list]\n",
    "            if gt_files[i_check] in k:\n",
    "                score += float(20 - k.index(gt_files[i_check]))/20\n",
    "\n",
    "        return -score/2000\n",
    "    \n",
    "    matrices = get_matrices([\"best\", \"NN\"])\n",
    "    \n",
    "    def f(x):\n",
    "        simi = x[0]*matrices[0] + x[1]*matrices[1]\n",
    "        return get_accuracy_on_set(simi)\n",
    "\n",
    "    start_pos = np.array([0.5, 0.5])\n",
    "\n",
    "    res = minimize(f, start_pos, method='Nelder-Mead')\n",
    "    coef = res.x\n",
    "    print(coef)\n",
    "    print(-get_accuracy_on_set(coef[0]*matrices[0] + coef[1]*matrices[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('data/images_train/0.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=2,\n",
    "                          save_to_dir='Augmented', save_prefix='0', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(10000):\n",
    "    img = load_img('data/images_train/{}.jpg'.format(i))  # this is a PIL image\n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "    X_train.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(2000):\n",
    "    img = load_img('data/images_test/{}.jpg'.format(i))  # this is a PIL image\n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "    X_test.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = get_output(better=[True, \"adj&noun&verb\", 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 3, 224, 224).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, 224, 224).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 3400/10000 [=========>....................] - ETA: 860s - loss: 55.7338 - acc: 0.0000e+00 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(3, 224, 224), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "num_classes = y.shape[1]\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y, nb_epoch=5, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: KeysView(<HDF5 file \"resnet101_weights_tf.h5\" (mode r)>)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = 'resnet101_weights_tf.h5'\n",
    "f = h5py.File(filename, 'r')\n",
    "\n",
    "# List all groups\n",
    "print(\"Keys: %s\" % f.keys())\n",
    "a_group_key = list(f.keys())[0]\n",
    "\n",
    "# Get the data\n",
    "data = list(f[a_group_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        pool = x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, pool\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = resnet152(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "\n",
    "images = []\n",
    "for i in range(10000):\n",
    "    img = load_img('data/images_train/{}.jpg'.format(i))\n",
    "    im = img_to_array(img)  # this is a Numpy array with shape (3, 224, 224)\n",
    "\n",
    "    # Remove train image mean\n",
    "    im[:, :, 0] *= 1/255\n",
    "    im[:, :, 1] *= 1/255\n",
    "    im[:, :, 2] *= 1/255\n",
    "\n",
    "#     im[:, :, 0] /= (0.225*255)\n",
    "#     im[:, :, 1] /= (0.224*255)\n",
    "#     im[:, :, 2] /= (0.229*255)\n",
    "\n",
    "    # 0.229, 0.224, 0.225\n",
    "\n",
    "    # Insert a new dimension for the batch_size\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    images.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "[[-0.92519498 -0.3412374  -0.38561994 ..., -0.53054374  0.87640893\n",
      "   0.91623217]\n",
      " [-0.90027428 -0.35439476 -0.4624089  ..., -0.37270391  0.93826228\n",
      "   1.04420257]\n",
      " [-1.01363885 -0.45941165 -0.51658583 ..., -0.57188582  0.90786612\n",
      "   0.71820503]\n",
      " ..., \n",
      " [-0.82085675 -0.56219256 -0.48617685 ..., -0.48869446  0.85021102\n",
      "   0.87953234]\n",
      " [-0.99806684 -0.53592271 -0.47668898 ..., -0.53457242  0.81942558\n",
      "   0.86126089]\n",
      " [-0.83344954 -0.3882567  -0.57174158 ..., -0.25705409  0.90653998\n",
      "   0.74078786]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-990a692d7e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moutput_pool_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_pool_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_fc_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_pool_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output_fc_ls, output_pool_ls = [], []\n",
    "k = 0\n",
    "for im in images:\n",
    "    output_fc, output_pool = model(Variable(Tensor(im)))\n",
    "    output_fc_ls.append(output_fc.data.numpy().ravel())\n",
    "    output_pool_ls.append(output_pool.data.numpy().ravel())\n",
    "    k += 1\n",
    "    if k % 50 == 0:\n",
    "        print(k)\n",
    "\n",
    "output_fc_ls = np.array(output_fc_ls)\n",
    "output_pool_ls = np.array(output_pool_ls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n",
      "(10000, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(output_fc_ls.shape)\n",
    "print(output_pool_ls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(output_fc_ls, columns=list(range(len(output_fc_ls[0])))).to_csv(\"ResNet152_fc_train.csv\", \n",
    "                                                                          header=False, \n",
    "                                                                          index=False)\n",
    "pd.DataFrame(output_pool_ls, columns=list(range(len(output_pool_ls[0])))).to_csv(\"ResNet152_pool_train.csv\", \n",
    "                                                                          header=False, \n",
    "                                                                          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(2000):\n",
    "    img = load_img('data/images_test/{}.jpg'.format(i))\n",
    "    im = img_to_array(img)  # this is a Numpy array with shape (3, 224, 224)\n",
    "\n",
    "    # Remove train image mean\n",
    "    im[:, :, 0] *= 1/255\n",
    "    im[:, :, 1] *= 1/255\n",
    "    im[:, :, 2] *= 1/255\n",
    "\n",
    "    # Insert a new dimension for the batch_size\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    images.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_fc_ls, output_pool_ls = [], []\n",
    "k = 0\n",
    "for im in images:\n",
    "    output_fc, output_pool = model(Variable(Tensor(im)))\n",
    "    output_fc_ls.append(output_fc.data.numpy().ravel())\n",
    "    output_pool_ls.append(output_pool.data.numpy().ravel())\n",
    "    k += 1\n",
    "    if k % 50 == 0:\n",
    "        print(k)\n",
    "\n",
    "output_fc_ls = np.array(output_fc_ls)\n",
    "output_pool_ls = np.array(output_pool_ls)\n",
    "\n",
    "print(output_fc_ls.shape)\n",
    "print(output_pool_ls.shape)\n",
    "\n",
    "pd.DataFrame(output_fc_ls, columns=list(range(len(output_fc_ls[0])))).to_csv(\"ResNet152_fc_test.csv\", \n",
    "                                                                          header=False, \n",
    "                                                                          index=False)\n",
    "pd.DataFrame(output_pool_ls, columns=list(range(len(output_pool_ls[0])))).to_csv(\"ResNet152_pool_test.csv\", \n",
    "                                                                          header=False, \n",
    "                                                                          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
