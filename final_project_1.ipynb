{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/leafy/Downloads/data/descriptions_test/2000.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2d2c89ebeef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/leafy/Downloads/data/descriptions_test/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/leafy/Downloads/data/descriptions_test/2000.txt'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "a = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "sys.stdout = a\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    res = []\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "#        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "#        print wordnet_pos\n",
    "        if wordnet_pos=='v' or wordnet_pos=='a'or wordnet_pos=='n':\n",
    "#        if wordnet_pos=='v':\n",
    "            res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "#        else:\n",
    "#            print wordnet_pos\n",
    "    return res\n",
    "\n",
    "\n",
    "for index in range(10000):  \n",
    "    data=[]\n",
    "    for line in open(\"/Users/leafy/Downloads/data/descriptions_test/\" + str(index) + \".txt\"):\n",
    "        l = line.strip('\\n')\n",
    "        data.append(l)\n",
    "    data = np.array(data) \n",
    "    \n",
    "    data = np.char.lower(data)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "    #lemmatize\n",
    "        data[i]=\" \".join(lemmatize_sentence(data[i]))    \n",
    "    #Strip the stop words\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        s = []\n",
    "        for word in data[i].split():\n",
    "            try:\n",
    "                if word not in stopwords.words('english'):\n",
    "                    s.append(word)\n",
    "#                 else:\n",
    "#                     print(word)\n",
    "            except:\n",
    "                pass\n",
    "        data[i] = \" \".join(s)        \n",
    "       \n",
    "        #Strip punctuation\n",
    "    import string\n",
    "    for j in range(len(data)):\n",
    "        data[j] = data[j].translate(None, string.punctuation)\n",
    "    number='1234567890'\n",
    "    for j in range(len(data)):\n",
    "        data[j] = data[j].translate(None, number) \n",
    "#         print(data[i])\n",
    "    f = open('/Users/leafy/anaconda/final_project/CS5785-Fall-2017-Final-Exam/features_test_prepossesing_verb&adj&noun/' + str(index) + \".txt\", 'w+')\n",
    "    for i in range(len(data)):\n",
    "        for s in data[i].split(\" \"):\n",
    "            f.write(s + \" \")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First find keyword based matcing strategy.\n",
    "\n",
    "# Need to start reading form here - http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477706\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import csv\n",
    "\n",
    "import re\n",
    "\n",
    "data_path='data'\n",
    "tags_test='tags_test'\n",
    "file_categories = defaultdict(int)\n",
    "file_tags = defaultdict(int)\n",
    "for file_name in glob(join(data_path, tags_test, \"*.txt\")):\n",
    "    for line in open(file_name).readlines():\n",
    "        file_tags[line.strip().split(\":\")[0]] += 1\n",
    "        file_categories[line.strip().split(\":\")[1]] += 1\n",
    "\n",
    "labels = list(set(list(file_categories.keys()) + list(file_tags.keys())))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([labels])\n",
    "\n",
    "pattern = \"\\d.*txt\"\n",
    "\n",
    "rows = []\n",
    "for file_name in sorted(glob(join(data_path, tags_test, \"*.txt\"))):\n",
    "    image_set = set()\n",
    "    for line in open(file_name).readlines():\n",
    "        image_set = image_set.union(set([line.strip().split(\":\")[0], line.strip().split(\":\")[1]]))\n",
    "\n",
    "    rows.append([int(re.findall(pattern=pattern, string=file_name)[0].strip(\"\\.txt\"))] + list(mlb.transform([image_set])[0]))\n",
    "rows = sorted(rows, key=lambda x: x[0])\n",
    "\n",
    "csv.writer(open('processed_tags_test.csv', \"w\")).writerows([[\"Name\"] + list(mlb.classes_)] + rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  1.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tag_test = pd.read_csv(\"processed_tags_test.csv\")\n",
    "tag_test = tag_test.columns[1:]\n",
    "tag_dict = dict()\n",
    "index = 0\n",
    "for word in tag_test:\n",
    "    if word not in tag_dict:\n",
    "        tag_dict[word] = index\n",
    "        index += 1\n",
    "#print tag_dict\n",
    "#print tag_test\n",
    "tag_feat=[]\n",
    "with open('processed_tags_test.csv') as csvfile:\n",
    "    lines = csv.reader(csvfile)\n",
    "    for index,line in enumerate (lines):\n",
    "        if index!=0:\n",
    "            tag_feat.append(line[1:])\n",
    "tag_feat=np.array(tag_feat, dtype= float )\n",
    "data_test=[]\n",
    "for index in range(2000):  \n",
    "    data=[]\n",
    "    for line in open(\"features_test_prepossesing/\" + str(index) + \".txt\"):\n",
    "        l = line.strip('\\n')\n",
    "        data.append(l)\n",
    "#        data = np.array(data)\n",
    "    data_test.append(data)\n",
    "#data_test=np.array(data_test)\n",
    "\n",
    "test_feat = []\n",
    "word_count = len(tag_dict)\n",
    "for sentence in data_test:\n",
    "    cur=[]\n",
    "    for tag in tag_test:\n",
    "        cur.append(sentence[0].count(tag))\n",
    "    test_feat.append(cur)\n",
    "    \n",
    "    \n",
    "#for sentence in data_test:\n",
    "    \n",
    "#    cur = [0] * word_count\n",
    "#    for word in sentence[0].split():\n",
    "#        if word in tag_dict:\n",
    "#            cur[tag_dict[word]] += 1      \n",
    "#    test_feat.append(cur)\n",
    "test_feat = np.array(test_feat, dtype= float )\n",
    "#print test_feat\n",
    "#test_feat[1].shape\n",
    "#for word in tag_dict:\n",
    "#    print word.split()\n",
    "print tag_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13643343  0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.12890716 ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.11821773  0.          0.        ]\n",
      " [ 0.          0.98986007  0.         ...,  0.1420459   0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(norm='l2',use_idf=True,smooth_idf=False,sublinear_tf=False)\n",
    "test_feat = transformer.fit_transform(test_feat).toarray()\n",
    "tag_feat= transformer.fit_transform(tag_feat).toarray()\n",
    "print tag_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "simi=cdist(test_feat,tag_feat,metric='cosine')\n",
    "rows=[['Descritpion_ID', 'Top_20_Image_IDs']]\n",
    "for i in range (len(test_feat)):\n",
    "    buffer1=[]\n",
    "    buffer1=simi[i].argsort()[:20]\n",
    "    rows.append([str(i) + \".txt\", \" \".join([str(value) + \".jpg\" for value in buffer1])])\n",
    "csv.writer(open(\"submission_try.csv\", \"w\")).writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kate\n",
      "build\n",
      "cat\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
