{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "a = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "sys.stdout = a\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    res = []\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    return res\n",
    "\n",
    "\n",
    "for index in range(2):  \n",
    "    data=[]\n",
    "    for line in open(\"/Users/leafy/Downloads/data/descriptions_test/\" + str(index) + \".txt\"):\n",
    "        l = line.strip('\\n')\n",
    "        data.append(l)\n",
    "    data = np.array(data) \n",
    "    \n",
    "    data = np.char.lower(data)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "    #lemmatize\n",
    "        data[i]=\" \".join(lemmatize_sentence(data[i]))    \n",
    "    #Strip the stop words\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        s = []\n",
    "        for word in data[i].split():\n",
    "            try:\n",
    "                if word not in stopwords.words('english'):\n",
    "                    s.append(word)\n",
    "#                 else:\n",
    "#                     print(word)\n",
    "            except:\n",
    "                pass\n",
    "        data[i] = \" \".join(s)        \n",
    "       \n",
    "        #Strip punctuation\n",
    "    import string\n",
    "    for j in range(len(data)):\n",
    "        data[j] = data[j].translate(None, string.punctuation)\n",
    "    number='1234567890'\n",
    "    for j in range(len(data)):\n",
    "        data[j] = data[j].translate(None, number) \n",
    "#         print(data[i])\n",
    "    f = open('/Users/leafy/anaconda/final_project/CS5785-Fall-2017-Final-Exam/features_test_prepossesing/' + str(index) + \".txt\", 'w+')\n",
    "    for i in range(len(data)):\n",
    "        for s in data[i].split(\" \"):\n",
    "            f.write(s + \" \")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find keyword based matcing strategy.\n",
    "\n",
    "# Need to start reading form here - http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477706\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import csv\n",
    "\n",
    "import re\n",
    "\n",
    "data_path='data'\n",
    "tags_test='tags_test'\n",
    "file_categories = defaultdict(int)\n",
    "file_tags = defaultdict(int)\n",
    "for file_name in glob(join(data_path, tags_test, \"*.txt\")):\n",
    "    for line in open(file_name).readlines():\n",
    "        file_tags[line.strip().split(\":\")[0]] += 1\n",
    "        file_categories[line.strip().split(\":\")[1]] += 1\n",
    "\n",
    "labels = list(set(list(file_categories.keys()) + list(file_tags.keys())))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([labels])\n",
    "\n",
    "pattern = \"\\d.*txt\"\n",
    "\n",
    "rows = []\n",
    "for file_name in sorted(glob(join(data_path, tags_test, \"*.txt\"))):\n",
    "    image_set = set()\n",
    "    for line in open(file_name).readlines():\n",
    "        image_set = image_set.union(set([line.strip().split(\":\")[0], line.strip().split(\":\")[1]]))\n",
    "\n",
    "    rows.append([int(re.findall(pattern=pattern, string=file_name)[0].strip(\"\\.txt\"))] + list(mlb.transform([image_set])[0]))\n",
    "rows = sorted(rows, key=lambda x: x[0])\n",
    "\n",
    "csv.writer(open('processed_tags_test.csv', \"w\")).writerows([[\"Name\"] + list(mlb.classes_)] + rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'accessory', u'airplane', u'animal', u'apple', u'appliance',\n",
       "       u'backpack', u'banana', u'baseball bat', u'baseball glove', u'bear',\n",
       "       u'bed', u'bench', u'bicycle', u'bird', u'boat', u'book', u'bottle',\n",
       "       u'bowl', u'broccoli', u'bus', u'cake', u'car', u'carrot', u'cat',\n",
       "       u'cell phone', u'chair', u'clock', u'couch', u'cow', u'cup',\n",
       "       u'dining table', u'dog', u'donut', u'electronic', u'elephant',\n",
       "       u'fire hydrant', u'food', u'fork', u'frisbee', u'furniture', u'giraffe',\n",
       "       u'hair drier', u'handbag', u'horse', u'hot dog', u'indoor', u'keyboard',\n",
       "       u'kitchen', u'kite', u'knife', u'laptop', u'microwave', u'motorcycle',\n",
       "       u'mouse', u'orange', u'outdoor', u'oven', u'parking meter', u'person',\n",
       "       u'pizza', u'potted plant', u'refrigerator', u'remote', u'sandwich',\n",
       "       u'scissors', u'sheep', u'sink', u'skateboard', u'skis', u'snowboard',\n",
       "       u'spoon', u'sports', u'sports ball', u'stop sign', u'suitcase',\n",
       "       u'surfboard', u'teddy bear', u'tennis racket', u'tie', u'toaster',\n",
       "       u'toilet', u'toothbrush', u'traffic light', u'train', u'truck', u'tv',\n",
       "       u'umbrella', u'vase', u'vehicle', u'wine glass', u'zebra'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tags = pd.read_csv(\"/Users/leafy/anaconda/processed_tags.csv\")\n",
    "test_tags=data.columns[1:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('test.txt', 'w')\n",
    "f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kate\n",
      "build\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "date=['kate  build  cat']\n",
    "date = np.array(date) \n",
    "for word in date[0].split():\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
